{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def load_and_preprocess_audio(file_path):\n",
        "    \"\"\"\n",
        "    加载音频文件并进行预处理\n",
        "    \"\"\"\n",
        "    # 加载音频\n",
        "    audio, _ = librosa.load(file_path, sr=SAMPLE_RATE) # , duration=DURATION\n",
        "\n",
        "    # 确保音频长度一致\n",
        "    if len(audio) < SAMPLE_RATE * DURATION:\n",
        "        audio = np.pad(audio, (0, int(SAMPLE_RATE * DURATION) - len(audio)))\n",
        "    else:\n",
        "        audio = audio[:int(SAMPLE_RATE * DURATION)]\n",
        "\n",
        "    # 提取梅尔频谱特征\n",
        "    mel_spec = librosa.feature.melspectrogram(\n",
        "        y=audio,\n",
        "        sr=SAMPLE_RATE,\n",
        "        n_mels=N_MELS,\n",
        "        n_fft=N_FFT,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "\n",
        "    # 转换为分贝单位\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    # 归一化\n",
        "    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / mel_spec_db.std()\n",
        "\n",
        "    # 确保数据类型匹配模型输入\n",
        "    # mel_spec_db = mel_spec_db.astype(np.float32)\n",
        "\n",
        "    return mel_spec_db\n",
        "\n",
        "def prepare_dataset(speech_dir, noise_dir):\n",
        "    \"\"\"\n",
        "    准备训练数据集\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # 加载语音数据\n",
        "    for file_name in os.listdir(speech_dir):\n",
        "        # if file_name.endswith('.wav'):\n",
        "        feature = load_and_preprocess_audio(os.path.join(speech_dir, file_name))\n",
        "        if np.isnan(feature).any():\n",
        "            print(f\"Skipping file {file_name} due to NaN values.\")\n",
        "            continue\n",
        "        features.append(feature)\n",
        "        labels.append(1)  # 1表示有语音\n",
        "\n",
        "    # 加载噪声数据\n",
        "    for file_name in os.listdir(noise_dir):\n",
        "        # if file_name.endswith('.wav'):\n",
        "        feature = load_and_preprocess_audio(os.path.join(noise_dir, file_name))\n",
        "        if np.isnan(feature).any():\n",
        "            print(f\"Skipping file {file_name} due to NaN values.\")\n",
        "            continue\n",
        "        features.append(feature)\n",
        "        labels.append(0)  # 0表示无语音\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# 原始的模型\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(N_MELS, 32, 1)),\n",
        "\n",
        "        # 添加L2正则化到卷积层\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
        "                             kernel_regularizer=tf.keras.regularizers.l2(0.0001) ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(0.0001) ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.25),  # 在卷积层后添加Dropout\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(0.0001) ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.25),  # 在卷积层后添加Dropout\n",
        "\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(0.001) ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "        # 增加更多的全连接层\n",
        "        tf.keras.layers.Dense(64, activation='relu',\n",
        "                              ),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "        # 添加temperature scaling层\n",
        "        tf.keras.layers.Dense(1, activation=None),\n",
        "        tf.keras.layers.Lambda(lambda x: x / 2.0),\n",
        "        tf.keras.layers.Activation('sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_model(model, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    训练模型\n",
        "    \"\"\"\n",
        "    # 使用学习率衰减\n",
        "    initial_learning_rate = 0.001\n",
        "    decay_steps = 1000\n",
        "    decay_rate = 0.9\n",
        "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate, decay_steps, decay_rate)\n",
        "\n",
        "    # 使用更好的优化器\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 添加早停以防止过拟合\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # 训练模型\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=5,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def convert_to_tflite(model, output_path):\n",
        "    \"\"\"\n",
        "    将模型转换为TFLite格式，支持动态输入维度\n",
        "    \"\"\"\n",
        "\n",
        "    def preprocess_audio(audio_path):\n",
        "      \"\"\"预处理音频文件\"\"\"\n",
        "      # 加载音频\n",
        "      audio, _ = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        # 确保音频长度一致\n",
        "      if len(audio) < 16000:\n",
        "          audio = np.pad(audio, (0, int(16000 * 1) - len(audio)))\n",
        "      else:\n",
        "          audio = audio[:int(16000)]\n",
        "\n",
        "\n",
        "      # 提取梅尔频谱特征\n",
        "      mel_spec = librosa.feature.melspectrogram(\n",
        "          y=audio,\n",
        "          sr=SAMPLE_RATE,\n",
        "          n_mels=N_MELS,\n",
        "          n_fft=N_FFT,\n",
        "          hop_length=HOP_LENGTH\n",
        "      )\n",
        "\n",
        "      # 转换为分贝单位\n",
        "      mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "      # 归一化\n",
        "      mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / mel_spec_db.std()\n",
        "\n",
        "      # 添加batch和channel维度\n",
        "      mel_spec_db = np.expand_dims(mel_spec_db, axis=[0, -1])\n",
        "      # 确保数据类型匹配模型输入\n",
        "      mel_spec_db = mel_spec_db.astype(np.float32)\n",
        "\n",
        "      return mel_spec_db\n",
        "\n",
        "    def representative_dataset_gen():\n",
        "      # 准备校准数据集\n",
        "      # 这里需要准备一些具有代表性的输入数据\n",
        "      paths = os.listdir('/content/data')\n",
        "      for path in paths:\n",
        "          # 生成随机数据作为示例\n",
        "          # 请根据实际模型的输入要求修改shape和数据范围\n",
        "          data = preprocess_audio(f'/content/data/{path}')\n",
        "          yield [data]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "    # 设置完全整数量化\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_dataset_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "    # 强制所有操作使用INT8\n",
        "    converter.inference_input_type = tf.int8  # ---------------------\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    # 执行转换\n",
        "    int8_model = converter.convert()\n",
        "\n",
        "    # 保存完全整数量化后的模型\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        f.write(int8_model)\n",
        "\n",
        "def inference(audio_path, model):\n",
        "    \"\"\"\n",
        "    使用模型进行推理\n",
        "    \"\"\"\n",
        "    # 预处理音频\n",
        "    feature = load_and_preprocess_audio(audio_path)\n",
        "    feature = np.expand_dims(feature, axis=[0, -1])  # 添加batch和channel维度\n",
        "    # 预测\n",
        "    prediction = model.predict(feature)\n",
        "    print(prediction)\n",
        "    return prediction[0][0]  # 返回预测概率\n"
      ],
      "metadata": {
        "id": "Ov68uxgqzHQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0. 配置参数\n",
        "SAMPLE_RATE = 16000  # 采样率\n",
        "DURATION = 1  # 每个音频片段的持续时间（秒）\n",
        "N_MELS = 40  # 梅尔频谱的频率维度\n",
        "HOP_LENGTH = 512  # STFT的跳跃长度\n",
        "N_FFT = 2048  # FFT窗口大小"
      ],
      "metadata": {
        "id": "D8kM8fwYO-OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 准备数据\n",
        "speech_dir = f\"/content/1.0s\"  # 包含语音的音频文件夹\n",
        "noise_dir = f\"/content/silence_data_1.0s\"    # 包含噪声的音频文件夹\n",
        "\n",
        "features, labels = prepare_dataset(speech_dir, noise_dir)\n",
        "features = np.expand_dims(features, axis=-1)\n",
        "# 分割训练集和验证集\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "mrHKZstAPNlm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 创建和训练模型\n",
        "model = create_model()\n",
        "history = train_model(model, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3vWyivg7PPqF",
        "outputId": "9fc46141-2e31-447b-c5b8-42e7ce8a1b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 43ms/step - accuracy: 0.9485 - loss: 0.1644 - val_accuracy: 0.9217 - val_loss: 0.2526\n",
            "Epoch 2/5\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.9935 - loss: 0.0296 - val_accuracy: 0.9800 - val_loss: 0.0589\n",
            "Epoch 3/5\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.9973 - loss: 0.0140 - val_accuracy: 0.9967 - val_loss: 0.0099\n",
            "Epoch 4/5\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 42ms/step - accuracy: 0.9970 - loss: 0.0123 - val_accuracy: 0.9989 - val_loss: 0.0061\n",
            "Epoch 5/5\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 52ms/step - accuracy: 0.9972 - loss: 0.0132 - val_accuracy: 0.9961 - val_loss: 0.0117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 转换为TFLite格式\n",
        "convert_to_tflite(model, \"model/vad_model_1.0s.tflite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QxmEzX-nPTVl",
        "outputId": "3318c60c-9462-4425-e083-35084ab6faac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpgioq6qb3'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 20, 32, 1), dtype=tf.float32, name='keras_tensor_152')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135633669271648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633661375264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668693984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668692752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668685888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668686768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668688528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668689760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668686240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668690112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668696624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668689232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668697504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135633668692224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. tf模型测试推理\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "import IPython.display as disp\n",
        "import os\n",
        "import time\n",
        "\n",
        "class VADInference:\n",
        "    def __init__(self, model_path, duration):\n",
        "        # 模型参数\n",
        "        self.SAMPLE_RATE = 16000\n",
        "        self.DURATION = duration\n",
        "        self.N_MELS = 40\n",
        "        self.HOP_LENGTH = 512\n",
        "        self.N_FFT = 2048\n",
        "\n",
        "        # 加载TFLite模型\n",
        "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "        self.interpreter.allocate_tensors()\n",
        "\n",
        "        # 获取输入和输出张量的细节\n",
        "        self.input_details = self.interpreter.get_input_details()\n",
        "        self.output_details = self.interpreter.get_output_details()\n",
        "\n",
        "    def preprocess_audio(self, audio_path, start=None):\n",
        "        \"\"\"预处理音频文件\"\"\"\n",
        "        # 加载音频\n",
        "        audio, _ = librosa.load(audio_path, sr=self.SAMPLE_RATE)\n",
        "\n",
        "        # 确保音频长度一致\n",
        "        if len(audio) < self.SAMPLE_RATE * self.DURATION:\n",
        "            audio = np.pad(audio, (0, int(self.SAMPLE_RATE * self.DURATION) - len(audio)))\n",
        "        else:\n",
        "            audio = audio[int(16000*start):int(16000*(self.DURATION+start))]\n",
        "\n",
        "        disp.display(disp.Audio(audio, rate=16000))\n",
        "\n",
        "        # 提取梅尔频谱特征\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=audio,\n",
        "            sr=self.SAMPLE_RATE,\n",
        "            n_mels=self.N_MELS,\n",
        "            n_fft=self.N_FFT,\n",
        "            hop_length=self.HOP_LENGTH\n",
        "        )\n",
        "\n",
        "        # 转换为分贝单位\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "        # 归一化\n",
        "        mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / mel_spec_db.std()\n",
        "\n",
        "        # 添加batch和channel维度\n",
        "        mel_spec_db = np.expand_dims(mel_spec_db, axis=[0, -1])\n",
        "        # 确保数据类型匹配模型输入\n",
        "        mel_spec_db = mel_spec_db.astype(np.float32)\n",
        "\n",
        "        return mel_spec_db\n",
        "\n",
        "    def predict(self, audio_path, threshold=0.5, start=None):\n",
        "        \"\"\"对音频文件进行VAD预测\"\"\"\n",
        "        # 预处理音频\n",
        "        input_data = self.preprocess_audio(audio_path, start=start)\n",
        "\n",
        "        # 设置输入张量\n",
        "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
        "\n",
        "        # 运行推理\n",
        "        self.interpreter.invoke()\n",
        "\n",
        "        # 获取输出结果\n",
        "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
        "\n",
        "        # 获取预测概率\n",
        "        probability = output_data[0][0]\n",
        "\n",
        "        # 根据阈值判断是否有人声\n",
        "        has_voice = probability > threshold\n",
        "\n",
        "        return {\n",
        "            'probability': float(probability),\n",
        "            'has_voice': bool(has_voice)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "import torchaudio\n",
        "audio_file = 'path/to/your/audio.mp3'\n",
        "audio, sr = torchaudio.load(audio_file)\n",
        "for i in range(int(audio.shape[1]/sr)):\n",
        "  vad = VADInference(model_path=\"/content/model/vad_model_1.0s.tflite\", duration=1)\n",
        "  result = vad.predict(audio_path=audio_file, threshold=0.5, start=i)\n",
        "  print(result)\n",
        "\n"
      ],
      "metadata": {
        "id": "mo_E7bzoiNnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iD737IR3EBSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUTFnDpXEBXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}